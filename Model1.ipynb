{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a04136bb-e263-476d-83c7-40f0b4d1fdcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('doucments.txt', 'r') as f:\n",
    "  text = f.read().replace('\\n', '')\n",
    "\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b49d3-783e-4455-9890-15b45ce673f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Callback data from file txt and replace new line to countine line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "631f6b48-d82f-40fe-8001-1f5ee4b91c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "77ff505a-8c85-4234-a4db-5b7c88045625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "df9372d2-a08b-4f03-933c-33676ec32fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1 3\n",
      "['Deep learning is a form of machine learning, and machine learning is a subfield of artificial intelligence.A machine learning algorithm is fed data by a computer and uses statistical techniques to help it “learn” how to get progressively better at a task, without necessarily having been specifically programmed for that task.', 'Instead, ML algorithms use historical data as input to predict new output values.', 'To that end, ML consists of both supervised learning (where the expected output for the input is known thanks to labeled data sets) and unsupervised learning (where the expected outputs are unknown due to the use of unlabeled data sets).Deep learning is a type of machine learning that runs inputs through a biologically inspired neural network architecture.']\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(sentences)*0.7)\n",
    "valid_size = int(len(sentences)*0.1)\n",
    "\n",
    "\n",
    "\n",
    "train = sentences[:train_size]\n",
    "valid = sentences[train_size:train_size+valid_size]\n",
    "test = sentences[train_size+valid_size:]\n",
    "\n",
    "\n",
    "print(len(train), len(valid), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ab02f42b-91b2-4195-91a5-7a26e3bc173b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Broadly', 'speaking', ',', 'artificially']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(' '.join(train))\n",
    "tokens1 = word_tokenize(' '.join(test))\n",
    "# print(tokens)\n",
    "# print(len(tokens))\n",
    "# print(tokens[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "19ece99a-0de5-4b32-8359-70592ad34bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[',', 'AI', 'a', 'to', 'of', 'the', 'and', '.', 'is', '—', 'can', 'with', 'human', 'for', 'intelligence', 'as', '’', 'strong', 'machine', 'on', 'like', 'or', 'systems', 'cognitive', 'such', 'speech', 'playing', 'patterns', 'typically', 'learn', 'how', 'by', 'in', 'many', 'cases', 'an', 's', 'some', 'without', 'over', 'weak', 'artificial', 'general', 'that', 'been', 'This', 'from', 'be', 'applied', 'task']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter(tokens).most_common(50)\n",
    "\n",
    "print(len(word_freq))\n",
    "\n",
    "vocab = [w for w, _ in word_freq]\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "00728ffd-13dd-4f90-ac34-c7d838a0ae95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_tokens = [_ if _ in vocab else '<unk>' for _ in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6494e025-a5f8-4c44-be8e-0abd6fb390d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def head(lst, n):  \n",
    "    return lst[:n]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "17db61f3-2df8-4175-ae65-d33d2756f4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Unigram(new_tokens):\n",
    "    dis = [{\"work1\":'',\"value\":1}]\n",
    "    for i in range(len(new_tokens)-1):\n",
    "        found_match = False\n",
    "        for item in dis:\n",
    "            if new_tokens[i] == item[\"work1\"]:\n",
    "                item[\"value\"] += 1\n",
    "                found_match = True\n",
    "                break\n",
    "        if not found_match:\n",
    "            dis.append({\"work1\": new_tokens[i], \"value\": 1})\n",
    "    \n",
    "    \n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "09ec1a22-7e19-4742-bd79-7de019395423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "print(len(Unigram(new_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bd6d9ad8-fde8-457b-a593-81ec1e1bd770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Biagram(new_tokens):\n",
    "    dis = [{\"work1\":'','work2':'',\"value\":1}]\n",
    "    for i in range(len(new_tokens)-1):\n",
    "        found_match = False\n",
    "        for item in dis:\n",
    "            if new_tokens[i] == item[\"work1\"] and new_tokens[i+1] == item[\"work2\"]:\n",
    "                item[\"value\"] += 1\n",
    "                found_match = True\n",
    "                break\n",
    "        if not found_match:\n",
    "            dis.append({\"work1\": new_tokens[i], \"work2\": new_tokens[i+1], \"value\": 1})\n",
    "    \n",
    "    \n",
    "    return dis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fe6c1cd8-5ccf-469b-a0c3-1ad529d7f286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'work1': '', 'work2': '', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'value': 61}, {'work1': '<unk>', 'work2': ',', 'value': 9}, {'work1': ',', 'work2': '<unk>', 'value': 11}, {'work1': '<unk>', 'work2': 'systems', 'value': 1}, {'work1': 'systems', 'work2': 'can', 'value': 1}, {'work1': 'can', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': 'with', 'value': 2}, {'work1': 'with', 'work2': 'human', 'value': 1}, {'work1': 'human', 'work2': 'cognitive', 'value': 1}, {'work1': 'cognitive', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': '—', 'value': 3}, {'work1': '—', 'work2': 'such', 'value': 1}, {'work1': 'such', 'work2': 'as', 'value': 1}, {'work1': 'as', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': 'speech', 'value': 1}, {'work1': 'speech', 'work2': ',', 'value': 1}, {'work1': ',', 'work2': 'playing', 'value': 1}, {'work1': 'playing', 'work2': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'and', 'value': 4}, {'work1': 'and', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': 'patterns', 'value': 1}, {'work1': 'patterns', 'work2': '.', 'value': 1}, {'work1': '.', 'work2': '<unk>', 'value': 4}, {'work1': '<unk>', 'work2': 'typically', 'value': 2}, {'work1': 'typically', 'work2': 'learn', 'value': 1}, {'work1': 'learn', 'work2': 'how', 'value': 1}, {'work1': 'how', 'work2': 'to', 'value': 2}, {'work1': 'to', 'work2': '<unk>', 'value': 6}, {'work1': '<unk>', 'work2': 'by', 'value': 1}, {'work1': 'by', 'work2': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'of', 'value': 9}, {'work1': 'of', 'work2': '<unk>', 'value': 4}, {'work1': '<unk>', 'work2': 'for', 'value': 3}, {'work1': 'for', 'work2': 'patterns', 'value': 1}, {'work1': 'patterns', 'work2': 'to', 'value': 1}, {'work1': '<unk>', 'work2': 'in', 'value': 2}, {'work1': 'in', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': '.', 'value': 5}, {'work1': '<unk>', 'work2': 'many', 'value': 1}, {'work1': 'many', 'work2': 'cases', 'value': 1}, {'work1': 'cases', 'work2': ',', 'value': 1}, {'work1': '<unk>', 'work2': 'an', 'value': 1}, {'work1': 'an', 'work2': 'AI', 'value': 1}, {'work1': 'AI', 'work2': '’', 'value': 1}, {'work1': '’', 'work2': 's', 'value': 2}, {'work1': 's', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': 'some', 'value': 2}, {'work1': 'some', 'work2': 'AI', 'value': 1}, {'work1': 'AI', 'work2': 'systems', 'value': 1}, {'work1': 'systems', 'work2': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'to', 'value': 6}, {'work1': 'to', 'work2': 'learn', 'value': 1}, {'work1': 'learn', 'work2': 'without', 'value': 1}, {'work1': 'without', 'work2': '<unk>', 'value': 2}, {'work1': '—', 'work2': 'for', 'value': 1}, {'work1': 'for', 'work2': '<unk>', 'value': 1}, {'work1': ',', 'work2': 'by', 'value': 1}, {'work1': 'by', 'work2': 'playing', 'value': 1}, {'work1': 'playing', 'work2': 'a', 'value': 1}, {'work1': 'a', 'work2': '<unk>', 'value': 10}, {'work1': '<unk>', 'work2': 'over', 'value': 1}, {'work1': 'over', 'work2': 'and', 'value': 1}, {'work1': 'and', 'work2': 'over', 'value': 1}, {'work1': 'over', 'work2': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'the', 'value': 3}, {'work1': 'the', 'work2': '<unk>', 'value': 8}, {'work1': 'and', 'work2': 'how', 'value': 1}, {'work1': '<unk>', 'work2': 'is', 'value': 2}, {'work1': 'is', 'work2': '<unk>', 'value': 3}, {'work1': '<unk>', 'work2': 'AI', 'value': 7}, {'work1': 'AI', 'work2': '<unk>', 'value': 6}, {'work1': 'typically', 'work2': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'strong', 'value': 2}, {'work1': 'strong', 'work2': 'AI', 'value': 3}, {'work1': 'AI', 'work2': 'and', 'value': 1}, {'work1': 'and', 'work2': 'weak', 'value': 1}, {'work1': 'weak', 'work2': '<unk>', 'value': 1}, {'work1': 'AI', 'work2': ',', 'value': 4}, {'work1': '<unk>', 'work2': 'as', 'value': 1}, {'work1': 'as', 'work2': 'artificial', 'value': 1}, {'work1': 'artificial', 'work2': 'general', 'value': 2}, {'work1': 'general', 'work2': 'intelligence', 'value': 2}, {'work1': 'intelligence', 'work2': ',', 'value': 1}, {'work1': ',', 'work2': 'is', 'value': 1}, {'work1': 'is', 'work2': 'a', 'value': 2}, {'work1': 'a', 'work2': 'machine', 'value': 3}, {'work1': 'machine', 'work2': 'that', 'value': 1}, {'work1': 'that', 'work2': 'can', 'value': 2}, {'work1': '<unk>', 'work2': '’', 'value': 2}, {'work1': '<unk>', 'work2': 'been', 'value': 2}, {'work1': 'been', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': 'on', 'value': 3}, {'work1': 'on', 'work2': '—', 'value': 1}, {'work1': '—', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': 'like', 'value': 2}, {'work1': 'like', 'work2': 'a', 'value': 1}, {'work1': 'a', 'work2': 'human', 'value': 1}, {'work1': 'human', 'work2': 'can', 'value': 1}, {'work1': 'can', 'work2': '.', 'value': 1}, {'work1': '.', 'work2': 'This', 'value': 2}, {'work1': 'This', 'work2': 'is', 'value': 1}, {'work1': 'is', 'work2': 'the', 'value': 2}, {'work1': 'of', 'work2': 'AI', 'value': 2}, {'work1': ',', 'work2': 'like', 'value': 1}, {'work1': 'like', 'work2': 'the', 'value': 1}, {'work1': '<unk>', 'work2': 'from', 'value': 2}, {'work1': 'from', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': 'or', 'value': 1}, {'work1': 'or', 'work2': 'the', 'value': 1}, {'work1': 'This', 'work2': '<unk>', 'value': 1}, {'work1': '’', 'work2': '<unk>', 'value': 1}, {'work1': 'of', 'work2': 'a', 'value': 1}, {'work1': 'machine', 'work2': 'with', 'value': 2}, {'work1': 'with', 'work2': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': 'intelligence', 'value': 1}, {'work1': 'intelligence', 'work2': 'that', 'value': 1}, {'work1': 'can', 'work2': 'be', 'value': 1}, {'work1': 'be', 'work2': 'applied', 'value': 1}, {'work1': 'applied', 'work2': 'to', 'value': 2}, {'work1': '<unk>', 'work2': 'task', 'value': 2}, {'work1': 'task', 'work2': 'is', 'value': 1}, {'work1': 'for', 'work2': 'many', 'value': 1}, {'work1': 'many', 'work2': 'AI', 'value': 1}, {'work1': 'for', 'work2': 'artificial', 'value': 1}, {'work1': 'intelligence', 'work2': '<unk>', 'value': 1}, {'work1': 'some', 'work2': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'be', 'value': 1}, {'work1': 'be', 'work2': '<unk>', 'value': 1}, {'work1': 'to', 'work2': 'the', 'value': 1}, {'work1': '<unk>', 'work2': 'a', 'value': 5}, {'work1': 'AI', 'work2': 'without', 'value': 1}, {'work1': 'to', 'work2': 'weak', 'value': 1}, {'work1': 'weak', 'work2': 'AI', 'value': 1}, {'work1': ',', 'work2': 'strong', 'value': 1}, {'work1': 'with', 'work2': 'a', 'value': 1}, {'work1': 'of', 'work2': 'cognitive', 'value': 1}, {'work1': '—', 'work2': 'and', 'value': 1}, {'work1': 'and', 'work2': 'an', 'value': 1}, {'work1': 'an', 'work2': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'cases', 'value': 1}, {'work1': 'cases', 'work2': '—', 'value': 1}, {'work1': '<unk>', 'work2': 'such', 'value': 1}, {'work1': 'such', 'work2': 'a', 'value': 1}, {'work1': 'to', 'work2': 'as', 'value': 1}, {'work1': 'AI', 'work2': 'or', 'value': 1}, {'work1': 'or', 'work2': '<unk>', 'value': 2}, {'work1': 'and', 'work2': 'is', 'value': 1}, {'work1': 'of', 'work2': 'human', 'value': 1}, {'work1': 'human', 'work2': 'intelligence', 'value': 1}, {'work1': 'intelligence', 'work2': 'applied', 'value': 1}, {'work1': 'to', 'work2': 'a', 'value': 1}, {'work1': 'like', 'work2': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'human', 'value': 1}, {'work1': 'human', 'work2': 'speech', 'value': 1}, {'work1': 'speech', 'work2': 'or', 'value': 1}, {'work1': 'on', 'work2': 'a', 'value': 1}, {'work1': 'AI', 'work2': 'is', 'value': 1}, {'work1': 'on', 'work2': '<unk>', 'value': 1}, {'work1': 'task', 'work2': '<unk>', 'value': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(Biagram(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "369e4320-1a39-42f0-8a9b-1857c11db93d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Trigram(new_tokens):\n",
    "    dis1 = [{\"work1\":'','work2':'','work3':'',\"value\":1}]\n",
    "    for i in range(len(new_tokens)-2):\n",
    "        found_match = False\n",
    "        for item in dis1:\n",
    "            if new_tokens[i] == item[\"work1\"] and new_tokens[i+1] == item[\"work2\"] and new_tokens[i+2] == item[\"work3\"]:\n",
    "                item[\"value\"] += 1\n",
    "                found_match = True\n",
    "                break\n",
    "        if not found_match:\n",
    "            dis1.append({\"work1\": new_tokens[i], \"work2\": new_tokens[i+1],'work3': new_tokens[i+2], \"value\": 1})\n",
    "            \n",
    "    return dis1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e0f8799c-dd18-472b-9f50-41b3d942f48b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'work1': '', 'work2': '', 'work3': '', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': ',', 'value': 2}, {'work1': '<unk>', 'work2': ',', 'work3': '<unk>', 'value': 7}, {'work1': ',', 'work2': '<unk>', 'work3': '<unk>', 'value': 6}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'systems', 'value': 1}, {'work1': '<unk>', 'work2': 'systems', 'work3': 'can', 'value': 1}, {'work1': 'systems', 'work2': 'can', 'work3': '<unk>', 'value': 1}, {'work1': 'can', 'work2': '<unk>', 'work3': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': '<unk>', 'work3': '<unk>', 'value': 26}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'with', 'value': 1}, {'work1': '<unk>', 'work2': 'with', 'work3': 'human', 'value': 1}, {'work1': 'with', 'work2': 'human', 'work3': 'cognitive', 'value': 1}, {'work1': 'human', 'work2': 'cognitive', 'work3': '<unk>', 'value': 1}, {'work1': 'cognitive', 'work2': '<unk>', 'work3': '—', 'value': 2}, {'work1': '<unk>', 'work2': '—', 'work3': 'such', 'value': 1}, {'work1': '—', 'work2': 'such', 'work3': 'as', 'value': 1}, {'work1': 'such', 'work2': 'as', 'work3': '<unk>', 'value': 1}, {'work1': 'as', 'work2': '<unk>', 'work3': 'speech', 'value': 1}, {'work1': '<unk>', 'work2': 'speech', 'work3': ',', 'value': 1}, {'work1': 'speech', 'work2': ',', 'work3': 'playing', 'value': 1}, {'work1': ',', 'work2': 'playing', 'work3': '<unk>', 'value': 1}, {'work1': 'playing', 'work2': '<unk>', 'work3': 'and', 'value': 1}, {'work1': '<unk>', 'work2': 'and', 'work3': '<unk>', 'value': 2}, {'work1': 'and', 'work2': '<unk>', 'work3': 'patterns', 'value': 1}, {'work1': '<unk>', 'work2': 'patterns', 'work3': '.', 'value': 1}, {'work1': 'patterns', 'work2': '.', 'work3': '<unk>', 'value': 1}, {'work1': '.', 'work2': '<unk>', 'work3': 'typically', 'value': 1}, {'work1': '<unk>', 'work2': 'typically', 'work3': 'learn', 'value': 1}, {'work1': 'typically', 'work2': 'learn', 'work3': 'how', 'value': 1}, {'work1': 'learn', 'work2': 'how', 'work3': 'to', 'value': 1}, {'work1': 'how', 'work2': 'to', 'work3': '<unk>', 'value': 2}, {'work1': 'to', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'by', 'value': 1}, {'work1': '<unk>', 'work2': 'by', 'work3': '<unk>', 'value': 1}, {'work1': 'by', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'of', 'value': 5}, {'work1': '<unk>', 'work2': 'of', 'work3': '<unk>', 'value': 4}, {'work1': 'of', 'work2': '<unk>', 'work3': ',', 'value': 1}, {'work1': ',', 'work2': '<unk>', 'work3': 'for', 'value': 1}, {'work1': '<unk>', 'work2': 'for', 'work3': 'patterns', 'value': 1}, {'work1': 'for', 'work2': 'patterns', 'work3': 'to', 'value': 1}, {'work1': 'patterns', 'work2': 'to', 'work3': '<unk>', 'value': 1}, {'work1': 'to', 'work2': '<unk>', 'work3': 'in', 'value': 1}, {'work1': '<unk>', 'work2': 'in', 'work3': '<unk>', 'value': 2}, {'work1': 'in', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': '.', 'value': 4}, {'work1': '<unk>', 'work2': '.', 'work3': '<unk>', 'value': 3}, {'work1': '.', 'work2': '<unk>', 'work3': 'many', 'value': 1}, {'work1': '<unk>', 'work2': 'many', 'work3': 'cases', 'value': 1}, {'work1': 'many', 'work2': 'cases', 'work3': ',', 'value': 1}, {'work1': 'cases', 'work2': ',', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'an', 'value': 1}, {'work1': '<unk>', 'work2': 'an', 'work3': 'AI', 'value': 1}, {'work1': 'an', 'work2': 'AI', 'work3': '’', 'value': 1}, {'work1': 'AI', 'work2': '’', 'work3': 's', 'value': 1}, {'work1': '’', 'work2': 's', 'work3': '<unk>', 'value': 2}, {'work1': 's', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'and', 'value': 2}, {'work1': 'and', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '.', 'work2': '<unk>', 'work3': 'some', 'value': 2}, {'work1': '<unk>', 'work2': 'some', 'work3': 'AI', 'value': 1}, {'work1': 'some', 'work2': 'AI', 'work3': 'systems', 'value': 1}, {'work1': 'AI', 'work2': 'systems', 'work3': '<unk>', 'value': 1}, {'work1': 'systems', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'to', 'value': 3}, {'work1': '<unk>', 'work2': 'to', 'work3': 'learn', 'value': 1}, {'work1': 'to', 'work2': 'learn', 'work3': 'without', 'value': 1}, {'work1': 'learn', 'work2': 'without', 'work3': '<unk>', 'value': 1}, {'work1': 'without', 'work2': '<unk>', 'work3': '—', 'value': 1}, {'work1': '<unk>', 'work2': '—', 'work3': 'for', 'value': 1}, {'work1': '—', 'work2': 'for', 'work3': '<unk>', 'value': 1}, {'work1': 'for', 'work2': '<unk>', 'work3': ',', 'value': 1}, {'work1': '<unk>', 'work2': ',', 'work3': 'by', 'value': 1}, {'work1': ',', 'work2': 'by', 'work3': 'playing', 'value': 1}, {'work1': 'by', 'work2': 'playing', 'work3': 'a', 'value': 1}, {'work1': 'playing', 'work2': 'a', 'work3': '<unk>', 'value': 1}, {'work1': 'a', 'work2': '<unk>', 'work3': '<unk>', 'value': 5}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'over', 'value': 1}, {'work1': '<unk>', 'work2': 'over', 'work3': 'and', 'value': 1}, {'work1': 'over', 'work2': 'and', 'work3': 'over', 'value': 1}, {'work1': 'and', 'work2': 'over', 'work3': '<unk>', 'value': 1}, {'work1': 'over', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'the', 'value': 2}, {'work1': '<unk>', 'work2': 'the', 'work3': '<unk>', 'value': 3}, {'work1': 'the', 'work2': '<unk>', 'work3': 'and', 'value': 1}, {'work1': '<unk>', 'work2': 'and', 'work3': 'how', 'value': 1}, {'work1': 'and', 'work2': 'how', 'work3': 'to', 'value': 1}, {'work1': 'to', 'work2': '<unk>', 'work3': 'is', 'value': 1}, {'work1': '<unk>', 'work2': 'is', 'work3': '<unk>', 'value': 2}, {'work1': 'is', 'work2': '<unk>', 'work3': 'to', 'value': 1}, {'work1': '<unk>', 'work2': 'to', 'work3': '<unk>', 'value': 2}, {'work1': 'to', 'work2': '<unk>', 'work3': ',', 'value': 1}, {'work1': ',', 'work2': '<unk>', 'work3': 'is', 'value': 1}, {'work1': 'is', 'work2': '<unk>', 'work3': 'AI', 'value': 1}, {'work1': '<unk>', 'work2': 'AI', 'work3': '<unk>', 'value': 1}, {'work1': 'AI', 'work2': '<unk>', 'work3': 'typically', 'value': 1}, {'work1': '<unk>', 'work2': 'typically', 'work3': '<unk>', 'value': 1}, {'work1': 'typically', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'strong', 'value': 1}, {'work1': '<unk>', 'work2': 'strong', 'work3': 'AI', 'value': 2}, {'work1': 'strong', 'work2': 'AI', 'work3': 'and', 'value': 1}, {'work1': 'AI', 'work2': 'and', 'work3': 'weak', 'value': 1}, {'work1': 'and', 'work2': 'weak', 'work3': '<unk>', 'value': 1}, {'work1': 'weak', 'work2': '<unk>', 'work3': 'AI', 'value': 1}, {'work1': '<unk>', 'work2': 'AI', 'work3': ',', 'value': 3}, {'work1': 'AI', 'work2': ',', 'work3': '<unk>', 'value': 3}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'as', 'value': 1}, {'work1': '<unk>', 'work2': 'as', 'work3': 'artificial', 'value': 1}, {'work1': 'as', 'work2': 'artificial', 'work3': 'general', 'value': 1}, {'work1': 'artificial', 'work2': 'general', 'work3': 'intelligence', 'value': 2}, {'work1': 'general', 'work2': 'intelligence', 'work3': ',', 'value': 1}, {'work1': 'intelligence', 'work2': ',', 'work3': 'is', 'value': 1}, {'work1': ',', 'work2': 'is', 'work3': 'a', 'value': 1}, {'work1': 'is', 'work2': 'a', 'work3': 'machine', 'value': 1}, {'work1': 'a', 'work2': 'machine', 'work3': 'that', 'value': 1}, {'work1': 'machine', 'work2': 'that', 'work3': 'can', 'value': 1}, {'work1': 'that', 'work2': 'can', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': '’', 'value': 1}, {'work1': '<unk>', 'work2': '’', 'work3': 's', 'value': 1}, {'work1': 's', 'work2': '<unk>', 'work3': 'been', 'value': 1}, {'work1': '<unk>', 'work2': 'been', 'work3': '<unk>', 'value': 2}, {'work1': 'been', 'work2': '<unk>', 'work3': 'to', 'value': 1}, {'work1': 'to', 'work2': '<unk>', 'work3': 'on', 'value': 1}, {'work1': '<unk>', 'work2': 'on', 'work3': '—', 'value': 1}, {'work1': 'on', 'work2': '—', 'work3': '<unk>', 'value': 1}, {'work1': '—', 'work2': '<unk>', 'work3': 'like', 'value': 1}, {'work1': '<unk>', 'work2': 'like', 'work3': 'a', 'value': 1}, {'work1': 'like', 'work2': 'a', 'work3': 'human', 'value': 1}, {'work1': 'a', 'work2': 'human', 'work3': 'can', 'value': 1}, {'work1': 'human', 'work2': 'can', 'work3': '.', 'value': 1}, {'work1': 'can', 'work2': '.', 'work3': 'This', 'value': 1}, {'work1': '.', 'work2': 'This', 'work3': 'is', 'value': 1}, {'work1': 'This', 'work2': 'is', 'work3': 'the', 'value': 1}, {'work1': 'is', 'work2': 'the', 'work3': '<unk>', 'value': 2}, {'work1': 'the', 'work2': '<unk>', 'work3': 'of', 'value': 2}, {'work1': '<unk>', 'work2': 'of', 'work3': 'AI', 'value': 2}, {'work1': 'of', 'work2': 'AI', 'work3': '<unk>', 'value': 2}, {'work1': 'AI', 'work2': '<unk>', 'work3': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'in', 'value': 1}, {'work1': 'in', 'work2': '<unk>', 'work3': ',', 'value': 1}, {'work1': '<unk>', 'work2': ',', 'work3': 'like', 'value': 1}, {'work1': ',', 'work2': 'like', 'work3': 'the', 'value': 1}, {'work1': 'like', 'work2': 'the', 'work3': '<unk>', 'value': 1}, {'work1': 'the', 'work2': '<unk>', 'work3': 'from', 'value': 1}, {'work1': '<unk>', 'work2': 'from', 'work3': '<unk>', 'value': 2}, {'work1': 'from', 'work2': '<unk>', 'work3': 'or', 'value': 1}, {'work1': '<unk>', 'work2': 'or', 'work3': 'the', 'value': 1}, {'work1': 'or', 'work2': 'the', 'work3': '<unk>', 'value': 1}, {'work1': 'the', 'work2': '<unk>', 'work3': '<unk>', 'value': 3}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'from', 'value': 1}, {'work1': 'from', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '.', 'work3': 'This', 'value': 1}, {'work1': '.', 'work2': 'This', 'work3': '<unk>', 'value': 1}, {'work1': 'This', 'work2': '<unk>', 'work3': 'of', 'value': 1}, {'work1': 'AI', 'work2': '<unk>', 'work3': '’', 'value': 1}, {'work1': '<unk>', 'work2': '’', 'work3': '<unk>', 'value': 1}, {'work1': '’', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'of', 'work3': 'a', 'value': 1}, {'work1': 'of', 'work2': 'a', 'work3': 'machine', 'value': 1}, {'work1': 'a', 'work2': 'machine', 'work3': 'with', 'value': 2}, {'work1': 'machine', 'work2': 'with', 'work3': '<unk>', 'value': 1}, {'work1': 'with', 'work2': '<unk>', 'work3': 'intelligence', 'value': 1}, {'work1': '<unk>', 'work2': 'intelligence', 'work3': 'that', 'value': 1}, {'work1': 'intelligence', 'work2': 'that', 'work3': 'can', 'value': 1}, {'work1': 'that', 'work2': 'can', 'work3': 'be', 'value': 1}, {'work1': 'can', 'work2': 'be', 'work3': 'applied', 'value': 1}, {'work1': 'be', 'work2': 'applied', 'work3': 'to', 'value': 1}, {'work1': 'applied', 'work2': 'to', 'work3': '<unk>', 'value': 1}, {'work1': 'to', 'work2': '<unk>', 'work3': 'task', 'value': 1}, {'work1': '<unk>', 'work2': 'task', 'work3': 'is', 'value': 1}, {'work1': 'task', 'work2': 'is', 'work3': 'the', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'for', 'value': 1}, {'work1': '<unk>', 'work2': 'for', 'work3': 'many', 'value': 1}, {'work1': 'for', 'work2': 'many', 'work3': 'AI', 'value': 1}, {'work1': 'many', 'work2': 'AI', 'work3': '<unk>', 'value': 1}, {'work1': 'AI', 'work2': '<unk>', 'work3': ',', 'value': 1}, {'work1': ',', 'work2': '<unk>', 'work3': 'the', 'value': 1}, {'work1': 'the', 'work2': '<unk>', 'work3': 'for', 'value': 1}, {'work1': '<unk>', 'work2': 'for', 'work3': 'artificial', 'value': 1}, {'work1': 'for', 'work2': 'artificial', 'work3': 'general', 'value': 1}, {'work1': 'general', 'work2': 'intelligence', 'work3': '<unk>', 'value': 1}, {'work1': 'intelligence', 'work2': '<unk>', 'work3': 'been', 'value': 1}, {'work1': 'been', 'work2': '<unk>', 'work3': 'with', 'value': 1}, {'work1': '<unk>', 'work2': 'with', 'work3': '<unk>', 'value': 1}, {'work1': 'with', 'work2': '<unk>', 'work3': '.', 'value': 1}, {'work1': '<unk>', 'work2': 'some', 'work3': '<unk>', 'value': 1}, {'work1': 'some', 'work2': '<unk>', 'work3': 'strong', 'value': 1}, {'work1': 'strong', 'work2': 'AI', 'work3': '<unk>', 'value': 2}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'be', 'value': 1}, {'work1': '<unk>', 'work2': 'be', 'work3': '<unk>', 'value': 1}, {'work1': 'be', 'work2': '<unk>', 'work3': ',', 'value': 1}, {'work1': ',', 'work2': '<unk>', 'work3': 'to', 'value': 1}, {'work1': '<unk>', 'work2': 'to', 'work3': 'the', 'value': 1}, {'work1': 'to', 'work2': 'the', 'work3': '<unk>', 'value': 1}, {'work1': 'of', 'work2': '<unk>', 'work3': 'a', 'value': 1}, {'work1': '<unk>', 'work2': 'a', 'work3': '<unk>', 'value': 4}, {'work1': 'a', 'work2': '<unk>', 'work3': 'AI', 'value': 2}, {'work1': '<unk>', 'work2': 'AI', 'work3': 'without', 'value': 1}, {'work1': 'AI', 'work2': 'without', 'work3': '<unk>', 'value': 1}, {'work1': 'without', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'to', 'work3': 'weak', 'value': 1}, {'work1': 'to', 'work2': 'weak', 'work3': 'AI', 'value': 1}, {'work1': 'weak', 'work2': 'AI', 'work3': ',', 'value': 1}, {'work1': 'AI', 'work2': ',', 'work3': 'strong', 'value': 1}, {'work1': ',', 'work2': 'strong', 'work3': 'AI', 'value': 1}, {'work1': 'AI', 'work2': '<unk>', 'work3': 'a', 'value': 1}, {'work1': '<unk>', 'work2': 'a', 'work3': 'machine', 'value': 1}, {'work1': 'machine', 'work2': 'with', 'work3': 'a', 'value': 1}, {'work1': 'with', 'work2': 'a', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'of', 'work3': 'cognitive', 'value': 1}, {'work1': 'of', 'work2': 'cognitive', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '—', 'work3': 'and', 'value': 1}, {'work1': '—', 'work2': 'and', 'work3': 'an', 'value': 1}, {'work1': 'and', 'work2': 'an', 'work3': '<unk>', 'value': 1}, {'work1': 'an', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': 'of', 'work2': '<unk>', 'work3': 'cases', 'value': 1}, {'work1': '<unk>', 'work2': 'cases', 'work3': '—', 'value': 1}, {'work1': 'cases', 'work2': '—', 'work3': '<unk>', 'value': 1}, {'work1': '—', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': 'of', 'work2': '<unk>', 'work3': 'such', 'value': 1}, {'work1': '<unk>', 'work2': 'such', 'work3': 'a', 'value': 1}, {'work1': 'such', 'work2': 'a', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'to', 'work3': 'as', 'value': 1}, {'work1': 'to', 'work2': 'as', 'work3': '<unk>', 'value': 1}, {'work1': 'as', 'work2': '<unk>', 'work3': 'AI', 'value': 1}, {'work1': '<unk>', 'work2': 'AI', 'work3': 'or', 'value': 1}, {'work1': 'AI', 'work2': 'or', 'work3': '<unk>', 'value': 1}, {'work1': 'or', 'work2': '<unk>', 'work3': 'AI', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'a', 'value': 1}, {'work1': '<unk>', 'work2': 'and', 'work3': 'is', 'value': 1}, {'work1': 'and', 'work2': 'is', 'work3': 'a', 'value': 1}, {'work1': 'is', 'work2': 'a', 'work3': '<unk>', 'value': 1}, {'work1': 'a', 'work2': '<unk>', 'work3': 'of', 'value': 1}, {'work1': '<unk>', 'work2': 'of', 'work3': 'human', 'value': 1}, {'work1': 'of', 'work2': 'human', 'work3': 'intelligence', 'value': 1}, {'work1': 'human', 'work2': 'intelligence', 'work3': 'applied', 'value': 1}, {'work1': 'intelligence', 'work2': 'applied', 'work3': 'to', 'value': 1}, {'work1': 'applied', 'work2': 'to', 'work3': 'a', 'value': 1}, {'work1': 'to', 'work2': 'a', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'like', 'value': 1}, {'work1': '<unk>', 'work2': 'like', 'work3': '<unk>', 'value': 1}, {'work1': 'like', 'work2': '<unk>', 'work3': 'a', 'value': 1}, {'work1': 'a', 'work2': '<unk>', 'work3': ',', 'value': 1}, {'work1': ',', 'work2': '<unk>', 'work3': 'human', 'value': 1}, {'work1': '<unk>', 'work2': 'human', 'work3': 'speech', 'value': 1}, {'work1': 'human', 'work2': 'speech', 'work3': 'or', 'value': 1}, {'work1': 'speech', 'work2': 'or', 'work3': '<unk>', 'value': 1}, {'work1': 'or', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'on', 'value': 2}, {'work1': '<unk>', 'work2': 'on', 'work3': 'a', 'value': 1}, {'work1': 'on', 'work2': 'a', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': '<unk>', 'work3': 'AI', 'value': 1}, {'work1': '<unk>', 'work2': 'AI', 'work3': 'is', 'value': 1}, {'work1': 'AI', 'work2': 'is', 'work3': '<unk>', 'value': 1}, {'work1': 'is', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}, {'work1': '<unk>', 'work2': 'on', 'work3': '<unk>', 'value': 1}, {'work1': 'on', 'work2': '<unk>', 'work3': 'a', 'value': 1}, {'work1': 'a', 'work2': '<unk>', 'work3': 'task', 'value': 1}, {'work1': '<unk>', 'work2': 'task', 'work3': '<unk>', 'value': 1}, {'work1': 'task', 'work2': '<unk>', 'work3': '<unk>', 'value': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(Trigram(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8331a104-7266-49d2-959d-5d5ccbbe9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fourgram(new_tokens):\n",
    "    dis2 = [{\"work1\":'','work2':'','work3':'','work4':'',\"value\":0}]\n",
    "    for i in range(len(new_tokens)-3):\n",
    "        found_match = False\n",
    "        for item in dis2:\n",
    "            if new_tokens[i] == item[\"work1\"] and new_tokens[i+1] == item[\"work2\"] and new_tokens[i+2] == item[\"work3\"] and new_tokens[i+3] == item[\"work4\"]:\n",
    "                item[\"value\"] += 1\n",
    "                found_match = True\n",
    "                break\n",
    "        if not found_match:\n",
    "            dis2.append({\"work1\": new_tokens[i], \"work2\": new_tokens[i+1],'work3': new_tokens[i+2],'work4': new_tokens[i+3], \"value\": 1})\n",
    "            \n",
    "    return dis2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9a083-e4fb-476e-9f8e-54e7219788d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "99fb9d5f-970d-4c07-930e-ce848142ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'work1': '<unk>', 'work2': '<unk>', 'work3': ',', 'value': 0.03278688524590164}\n",
      "{'work1': '<unk>', 'work2': '<unk>', 'work3': ',', 'work4': '<unk>', 'value': 1.0}\n",
      "{'work1': '<unk>', 'work2': '<unk>', 'value': 0.3961038961038961}\n",
      "{'work1': '<unk>', 'value': 0.43874643874643876}\n"
     ]
    }
   ],
   "source": [
    "# dictionari_Unigram = Unigram(new_tokens)\n",
    "# dictionari_Biagram = Biagram(new_tokens)\n",
    "# dictionari_Trigram = Trigram(new_tokens)\n",
    "# dictionari_Fourgram = Fourgram(new_tokens)\n",
    "\n",
    "\n",
    "def Probability_Trigram(new_token):\n",
    "    dictionari_Biagram = Biagram(new_tokens)\n",
    "    dictionari_Trigram = Trigram(new_tokens)\n",
    "    for x in dictionari_Trigram:\n",
    "        for y in dictionari_Biagram:\n",
    "            if x[\"work1\"] == y[\"work1\"] and x[\"work2\"] == y[\"work2\"]:\n",
    "                # print(x[\"work1\"],x[\"work2\"])\n",
    "                # print(x[\"value\"],y[\"value\"])\n",
    "                \n",
    "                x[\"value\"] = x[\"value\"]/y[\"value\"]\n",
    "    \n",
    "    return dictionari_Trigram\n",
    "\n",
    "print( Probability_Trigram(new_tokens)[1])\n",
    "\n",
    "def Probability_Fourgram(new_tokens):\n",
    "    dictionari_Trigram = Trigram(new_tokens)\n",
    "    dictionari_Fourgram = Fourgram(new_tokens)\n",
    "    \n",
    "    for x in dictionari_Fourgram:\n",
    "        for y in dictionari_Trigram:\n",
    "            if x[\"work1\"] == y[\"work1\"] and x[\"work2\"] == y[\"work2\"] and x[\"work3\"] == y[\"work3\"] :\n",
    "                # print(x[\"work1\"],x[\"work2\"])\n",
    "                # print(x[\"value\"],y[\"value\"])\n",
    "                x[\"value\"] = x[\"value\"]/y[\"value\"]\n",
    "    \n",
    "    return dictionari_Fourgram\n",
    "\n",
    "print(Probability_Fourgram(new_tokens)[1])\n",
    "\n",
    "def Probability_Bigram(new_tokens):\n",
    "    dictionari_Unigram = Unigram(new_tokens)\n",
    "    dictionari_Biagram = Biagram(new_tokens)\n",
    "    for x in dictionari_Biagram:\n",
    "        for y in dictionari_Unigram:\n",
    "            if x[\"work1\"] == y[\"work1\"]:\n",
    "                x[\"value\"] = x[\"value\"]/y[\"value\"]\n",
    "    \n",
    "    return dictionari_Biagram\n",
    "\n",
    "print(Probability_Bigram(new_tokens)[1])\n",
    "\n",
    "def Probability_Unigram(new_tokens):\n",
    "    dictionari_Unigram = Unigram(new_tokens)\n",
    "    for x in dictionari_Unigram:\n",
    "        x[\"value\"] = x[\"value\"]/len(new_tokens)\n",
    "    \n",
    "    return dictionari_Unigram\n",
    "\n",
    "print(Probability_Unigram(new_tokens)[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76780b5e-0426-496d-a9c1-7c50db5e87d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b9fe62-38dc-49f4-bc63-74d3d468833d",
   "metadata": {},
   "source": [
    "##Formula of Probabily sentence P(W) = P(w1,w2,...wn) = P(w1)xP(w2|w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "43dc2981-9522-4459-9156-54af300d6c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ML1(sentence,new_tokens):\n",
    "    \n",
    "    ##split data from sentence to array of word\n",
    "    sentences = sent_tokenize(sentence)\n",
    "    word = word_tokenize(' '.join(sentences))\n",
    "       \n",
    "    # dictionari_Unigram = Unigram(new_tokens)\n",
    "    # dictionari_Biagram = Biagram(new_tokens)\n",
    "    # dictionari_Trigram = Trigram(new_tokens)\n",
    "    # dictionari_Fourgram = Fourgram(new_tokens)\n",
    "    \n",
    "    \n",
    "    len_word = len(word)\n",
    "    probability_Unigram = 1\n",
    "    probability_bigram = 1\n",
    "    probability_trigram = 1\n",
    "    probability_fourgram = 1\n",
    "    probobality_nextword = [{\"last\":\"\",\"value\":0}]\n",
    "    if len_word >4:\n",
    "        maxPro = 0\n",
    "        maxPro_tri = 0\n",
    "        maxPro_bi =0\n",
    "        maxPro_ui=0\n",
    "        nextWord = ' '\n",
    "        lamda = 0.5\n",
    "                \n",
    "        for i in range(len(word)-3):\n",
    "            for x in Probability_Fourgram(new_tokens):\n",
    "                if x['work1'] == word[i] and x['work2'] == word[i+1] and x['work3'] == word[i+2] and  x['work4'] == word[i+3]:\n",
    "                    if x['value']>0:\n",
    "                        probability_fourgram *= x['value']\n",
    "                    else:\n",
    "                        for x in Probability_Trigram(new_tokens):\n",
    "                            if x['work1'] == word[i+1] and x['work2'] == word[i+2] and x['work3'] == word[i+3] and x['value']>0:\n",
    "                                probability_fourgram *= x['value']\n",
    "                            else:\n",
    "                                for x in  Probability_Bigram(new_tokens):\n",
    "                                    if x['work1'] == word[i+2] and x['work2'] == word[i+3] and x['value']>0:\n",
    "                                         probability_fourgram *= x['value']\n",
    "                                    else:\n",
    "                                        probability_fourgram == 0\n",
    "                                  \n",
    "                if x['work1'] == word[len(word)-3] and x['work2'] == word[len(word)-2] and x['work3'] == word[len(word)-1] and x['value']> maxPro and probability_fourgram>0:\n",
    "                    maxPro = x['value']\n",
    "                    nextWord = x[\"work4\"]\n",
    "                    \n",
    "        if  nextWord == '':\n",
    "            for x in Probability_Trigram(new_tokens):\n",
    "                    if x['work1'] == word[len(word)-2] and x['work2'] == word[len(word)-1] and x['value']>maxPro_tri:\n",
    "                        maxPro_tri = x['value']\n",
    "                        nextWord = x[\"work3\"]\n",
    "                    else:\n",
    "                        for x in  Probability_Bigram(new_tokens):\n",
    "                            if x['work1'] == word[len(word)-1] and x['value']>maxPro_bi:\n",
    "                                maxPro_bi =  x['value']\n",
    "                                nextWord = x['work2']\n",
    "                            else:\n",
    "                                nextWord = \"None!\"\n",
    "                                probability_fourgram == 0\n",
    "        if maxPro_tri>0:\n",
    "            probability_fourgram *= (maxPro_tri*lamda)\n",
    "        elif maxPro_bi>0:\n",
    "             probability_fourgram *= (maxPro_bi*lamda)\n",
    "\n",
    "        if(probability_fourgram>0):\n",
    "            print(\"success\")\n",
    "            return sentence+' '+nextWord\n",
    "        else:\n",
    "            print(\"faile\")\n",
    "            \n",
    "    elif len_word==4:\n",
    "        maxPro = 0\n",
    "        maxPro_tri = 0\n",
    "        maxPro_bi =0\n",
    "        nextWord = ' '\n",
    "        lamda = 0.5\n",
    "        ## check value of probability between sentence with dictionari of Bigram\n",
    "        for i in range(len(word)-3):\n",
    "            for x in Probability_Fourgram(new_tokens):\n",
    "                if x['work1'] == word[i] and x['work2'] == word[i+1] and x['work3'] == word[i+2] and  x['work4'] == word[i+3]:\n",
    "                    if x['value']>0:\n",
    "                        probability_fourgram *= x['value']\n",
    "                    else:\n",
    "                        for x in Probability_Trigram(new_tokens):\n",
    "                            if x['work1'] == word[i+1] and x['work2'] == word[i+2] and x['work3'] == word[i+3] and x['value']>0:\n",
    "                                probability_fourgram *= x['value']\n",
    "                            else:\n",
    "                                for x in  Probability_Bigram(new_tokens):\n",
    "                                    if x['work1'] == word[i+2] and x['work2'] == word[i+3] and x['value']>0:\n",
    "                                         probability_fourgram *= x['value']\n",
    "                                    else:\n",
    "                                        probability_fourgram == 0\n",
    "                if x['work1'] == word[i+1] and x['work2'] == word[i+2] and x['work3'] == word[i+3] and x['value']> maxPro and probability_fourgram>0:\n",
    "                    maxPro = x['value']\n",
    "                    nextWord = x[\"work4\"]\n",
    "                    \n",
    "        if  nextWord == '':\n",
    "            for x in Probability_Trigram(new_tokens):\n",
    "                    if x['work1'] == word[i+1] and x['work2'] == word[i+2] and x['value']>maxPro_tri:\n",
    "                        maxPro_tri =  x['value']\n",
    "                        nextWord = x[\"work3\"]\n",
    "                    else:\n",
    "                        for x in  Probability_Bigram(new_tokens):\n",
    "                            if x['work1'] == word[len(word)-1] and x['value']>maxPro_bi:\n",
    "                                maxPro_bi = x['value']\n",
    "                                nextWord = x['work2']\n",
    "                            else:\n",
    "                                nextWord = \"None!\"\n",
    "                                probability_fourgram == 0\n",
    "                    \n",
    "                    \n",
    "        if maxPro_tri>0:\n",
    "            probability_fourgram *= (maxPro_tri*lamda)\n",
    "        elif maxPro_bi>0:\n",
    "             probability_fourgram *= (maxPro_bi*lamda)\n",
    "\n",
    "        if(probability_fourgram>0):\n",
    "            print(\"success\")\n",
    "            return sentence+' '+nextWord\n",
    "        else:\n",
    "            print(\"faile\")\n",
    "    elif len_word == 3:\n",
    "        maxPro = 0\n",
    "        maxPro_tri = 0\n",
    "        maxPro_bi =0\n",
    "        nextWord = ' '\n",
    "        lamda = 0.5\n",
    "         ## check value of probability between sentence with dictionari of Bigram\n",
    "        for i in range(len(word)-2):\n",
    "            for x in Probability_Trigram(new_tokens):\n",
    "                if x['work1'] == word[i] and x['work2'] == word[i+1] and x['work3'] == word[i+2]:\n",
    "                    if x['value']>0:\n",
    "                        probability_trigram *= x['value']\n",
    "                    else:\n",
    "\n",
    "                        for x in  Probability_Bigram(new_tokens):\n",
    "                            if x['work1'] == word[i+1] and x['work2'] == word[i+2] and x['value']>0:\n",
    "                                 probability_trigram *= x['value']\n",
    "                            else:\n",
    "                                probability_trigram == 0\n",
    "                                        \n",
    "                if x['work1'] == word[i+1] and x['work2'] == word[i+2] and x['value']> maxPro_tri and probability_trigram>0:\n",
    "                    maxPro_tri = x['value']\n",
    "                    nextWord = x[\"work3\"]\n",
    "                    \n",
    "        if  nextWord == '':\n",
    "             for x in  Probability_Bigram(new_tokens):\n",
    "                if x['work1'] == word[i+1] and x['value']>maxPro_bi:\n",
    "                    maxPro_bi == x['value']\n",
    "                    nextWord = x['work2']\n",
    "                else:\n",
    "                    nextWord = \"None!\"\n",
    "                    probability_trigram == 0\n",
    "                    \n",
    "        if maxPro_bi>0:\n",
    "             probability_trigram *= (maxPro_bi*lamda)\n",
    "\n",
    "        if(probability_trigram>0):\n",
    "            print(\"success\")\n",
    "            return sentence+' '+nextWord\n",
    "        else:\n",
    "            print(\"faile\")\n",
    "      \n",
    "        return sentence+' '+nextWord\n",
    "        \n",
    "    elif len_word == 2:\n",
    "        maxPro = 0\n",
    "        maxPro_tri = 0\n",
    "        maxPro_bi =0\n",
    "        maxPro_ui=0\n",
    "        nextWord = ' '\n",
    "        lamda = 0.5\n",
    "        ## check value of probability between sentence with dictionari of Bigram\n",
    "        for i in range(len(word)-1):\n",
    "            for x in Probability_Bigram(new_tokens):\n",
    "                if x['work1'] == word[i] and x['work2'] == word[i+1]:\n",
    "                    if x['value']>0:\n",
    "                        probability_bigram *= x['value']\n",
    "                    else:\n",
    "                        for x in  Probability_Unigrame(new_tokens):\n",
    "                            if x['work1'] == word[i+1] and x['value']>0:\n",
    "                                 probability_bigram *= x['value']\n",
    "                            else:\n",
    "                                probability_bigram == 0\n",
    "                 # store probability of next word by using check all dictionary bigram that we have work1 equal last word on input\n",
    "                 # sentence\n",
    "                if x['work1'] == word[i+1] and x['value']> maxPro_ui and probability_bigram>0:\n",
    "                    maxPro_ui = x['value']\n",
    "                    nextWord = x[\"work2\"]\n",
    "        \n",
    "        if  nextWord == '':\n",
    "             for x in  Probability_Unigrame(new_tokens):\n",
    "                if  x['value']>maxPro_ui:\n",
    "                    maxPro_ui == x['value']\n",
    "                    nextWord = x['work1']\n",
    "                else:\n",
    "                    nextWord = \"None!\"\n",
    "                    probability_bigram == 0\n",
    "                    \n",
    "        if maxPro_ui>0:\n",
    "             probability_bigram *= (maxPro_ui*lamda)\n",
    "\n",
    "        if(probability_bigram>0):\n",
    "            print(\"success\")\n",
    "            return sentence+' '+nextWord\n",
    "        else:\n",
    "            print(\"faile\")\n",
    "      \n",
    "                \n",
    "        return sentence+' '+nextWord\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8cf72bfc-4275-4b99-bef3-d59897f8d401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3d1e8ac5-7ee8-4459-87ac-9944050055de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "\n",
      "\n",
      " predict the next word of <<  as artificial general intelligence ,  >>  =>  as artificial general intelligence , is\n"
     ]
    }
   ],
   "source": [
    "sentence = 'as artificial general intelligence ,'\n",
    "ML1(sentence,new_tokens)\n",
    "\n",
    "print(\"\\n\\n predict the next word of << \",sentence,\" >>  => \",ML1(sentence,new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ec5d4df4-4293-4a05-af34-93d8d3213f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Probability_Trigram_K(new_tokens,k,size_vocabulary):\n",
    "    dictionari_Biagram = Biagram(new_tokens)\n",
    "    dictionari_Trigram = Trigram(new_tokens)\n",
    "    for x in dictionari_Trigram:\n",
    "        for y in dictionari_Biagram:\n",
    "            if x[\"work1\"] == y[\"work1\"] and x[\"work2\"] == y[\"work2\"]:\n",
    "                x[\"value\"] = (x[\"value\"]+k)/(y[\"value\"]+(k*size_vocabulary))\n",
    "    \n",
    "    return dictionari_Trigram\n",
    "\n",
    "\n",
    "def Probability_Fourgram_K(new_tokens,k,size_vocabulary):\n",
    "    dictionari_Trigram = Trigram(new_tokens)\n",
    "    dictionari_Fourgram = Fourgram(new_tokens)\n",
    "    for x in dictionari_Fourgram:\n",
    "        for y in dictionari_Trigram:\n",
    "            if x[\"work1\"] == y[\"work1\"] and x[\"work2\"] == y[\"work2\"] and x[\"work3\"] == y[\"work3\"] :\n",
    "                x[\"value\"] = (x[\"value\"]+k)/(y[\"value\"]+(k*size_vocabulary))\n",
    "    \n",
    "    return dictionari_Fourgram\n",
    "\n",
    "def Probability_Bigram_K(new_tokens,k,size_vocabulary):\n",
    "    dictionari_Unigram = Unigram(new_tokens)\n",
    "    dictionari_Biagram = Biagram(new_tokens)\n",
    "    for x in dictionari_Biagram:\n",
    "        for y in dictionari_Unigram:\n",
    "            if x[\"work1\"] == y[\"work1\"]:\n",
    "                x[\"value\"] = (x[\"value\"]+k)/(y[\"value\"]+(k*size_vocabulary))\n",
    "    \n",
    "    return dictionari_Biagram\n",
    "    \n",
    "def Probability_Unigrame_K(new_tokens,k,size_vocabulary):\n",
    "     for x in dictionari_Unigram:\n",
    "        x[\"value\"] = (x[\"value\"]+k)/(len(token)+(k*size_vocabulary))\n",
    "    \n",
    "     return dictionari_Unigram\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7feb8e7e-6280-43b1-b6a4-28ac0284cf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ML2(sentence,new_tokens,k,size_vocabulary):\n",
    "    \n",
    "    ##split data from sentence to array of word\n",
    "    sentences = sent_tokenize(sentence)\n",
    "    word = word_tokenize(' '.join(sentences))\n",
    "    word = [_ if _ in vocab else '<unk>' for _ in word]\n",
    "    \n",
    "    print(word)\n",
    "     \n",
    "\n",
    "    \n",
    "    len_word = len(word)\n",
    "    probability_Unigram = 1\n",
    "    probability_bigram = 1\n",
    "    probability_trigram = 1\n",
    "    probability_fourgram = 1\n",
    "    probability_interpolation = 0\n",
    "    probobality_nextword = [{\"last\":\"\",\"value\":0}]\n",
    "\n",
    "    if len_word >4:\n",
    "        maxPro = 0\n",
    "        maxPro_tri = 0\n",
    "        maxPro_bi =0\n",
    "        maxPro_ui=0\n",
    "        nextWord = ' '\n",
    "        \n",
    "                \n",
    "        for i in range(len(word)-3):\n",
    "            for x in Probability_Fourgram_K(new_tokens,k,size_vocabulary):\n",
    "                if x['work1'] == word[i] and x['work2'] == word[i+1] and x['work3'] == word[i+2] and  x['work4'] == word[i+3]:\n",
    "                    probability_fourgram = x['value']\n",
    "                else:\n",
    "                    probability_fourgram = 0\n",
    "                    \n",
    "            for x in Probability_Trigram_K(dictionari_Biagram,dictionari_Trigram):\n",
    "                if x['work1'] == word[i+1] and x['work2'] == word[i+2] and x['work3'] == word[i+3] and x['value']>0:\n",
    "                    probability_trigram = x['value']\n",
    "                else:\n",
    "                    probability_trigram = 0\n",
    "\n",
    "            for x in  Probability_Bigram_K(dictionari_Unigram,dictionari_Biagram):\n",
    "                if x['work1'] == word[i+2] and x['work2'] == word[i+3] and x['value']>0:\n",
    "                     probability_bigram = x['value']\n",
    "                else:\n",
    "                    probability_bigram = 0\n",
    "\n",
    "            for x in   Probability_Unigrame_K(dictionari_Unigram,new_tokens):\n",
    "                if x['work1'] == word[i+3] and x['value']>0:\n",
    "                     probability_Unigram = x['value']\n",
    "                else:\n",
    "                    probability_Unigram = 0\n",
    "                            \n",
    "                    probability_interpolation = (lamda1*probability_fourgram) + (lamda2*probability_trigram) + (lamda3*probability_bigram) + (lamda4*probability_Unigram)\n",
    "                    probability_fourgram =* probability_interpolation        \n",
    "                             \n",
    "        for x in Probability_Fourgram_K(new_tokens,k,size_vocabulary):\n",
    "            if x['work1'] == word[len(word)-3] and x['work2'] == word[len(word)-2] and x['work3'] == word[len(word)-1]:\n",
    "                probability_fourgram = x['value']\n",
    "                \n",
    "                for y in Probability_Trigram_K(new_tokens,k,size_vocabulary):\n",
    "                    if y['work1'] == word[len(word)-2] and y['work2'] == word[len(word)-1] and y['work3'] == x['work4']:\n",
    "                        probability_trigram = y['value']\n",
    "                            \n",
    "                for z in Probability_Bigram_K(new_tokens,k,size_vocabulary):\n",
    "                    if z['work1'] == word[len(word)-1] and z['work2'] == x['work4']:\n",
    "                        probability_bigram = z['value']\n",
    "                        \n",
    "                for r in Probability_Unigrame_K(dictionari_Unigram,new_tokens):\n",
    "                    if r['work1'] == x['work4']\n",
    "                        probability_Unigram = r['value']\n",
    "                \n",
    "                 probability_interpolation = (lamda1*probability_fourgram) + (lamda2*probability_trigram) + (lamda3*probability_bigram) + (lamda4*probability_Unigram)\n",
    "                    \n",
    "                if maxPro < probability_interpolation:\n",
    "                    maxPro = probability_interpolation\n",
    "                    next_word = x['work4']\n",
    "                        \n",
    "                    \n",
    "#         if  nextWord == '':\n",
    "            \n",
    "#             # Add K smoothing\n",
    "                \n",
    "#         if maxPro_tri>0:\n",
    "#             probability_fourgram *= (maxPro_tri*lamda)\n",
    "#         elif maxPro_bi>0:\n",
    "#              probability_fourgram *= (maxPro_bi*lamda)\n",
    "\n",
    "#         if(probability_fourgram>0):\n",
    "#             print(\"success\")\n",
    "#             return sentence+' '+nextWord\n",
    "#         else:\n",
    "#             print(\"faile\")\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "29ec13cd-8341-4dc1-ae21-a1cfadf16bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'artificial', 'general', 'intelligence', ',']\n",
      "is\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ML2(\"as artificial general intelligence ,\",new_tokens,1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c14b2fa-3e47-4817-9db1-1718aae381d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1090588922.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    git clone https://gitlab.com/Kheanengly/nlp1.git\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f514de-3fd6-43bc-8679-c64248f6d0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
